1、如何处理不平衡数据?
通常，实际数据和标准数据集在数据集的属性和数据量方面差异很大(标准数据集不需要调整)。对于实际的数据集，可能会出现数据不平衡的情况，即类之间的数据不平衡。我们现在可以考虑以下技术：

a、选择正确的度量来评估模型：对于不平衡的数据集，使用准确性来评估是一项非常危险的工作。应选择精度、召回、F1 分数、AUC等合适的评价量。
b、重新采样训练数据集：除了使用不同的评估标准，人们还可以使用一些技术来获得不同的数据集。从一个不平衡的数据集中创建一个平衡的数据集有两种方法，即欠采样和过采样，具体技术包括重复、
   bootstrapping 或 hits(综合少数过采样技术)等方法。
c、许多不同模型的集成：通过创建更多的数据来概括模型在实践中并不总是可行的。例如，你有两个层，一个拥有 1000 个数据的罕见类，一个包含 10,000 个数据样本的大型类。因此，我们可以考虑一个 10 个模型的训练解决方案，而不是试图从一个罕见的类中找到 9000 个数据样本来进行模型训练。每个模型由 1000 个稀有类和 1000 个大规模类训练而成。然后使用集成技术获得优秀结果。
d、重新设计模型 — 损失函数：使用惩罚技术对代价函数中的多数类进行严厉惩罚，帮助模型本身更好地学习稀有类的数据。这使得损失函数的值在类中更全面。

一、什么是数据不平衡问题
数据不平衡也可称作数据倾斜。在实际应用中，数据集的样本特别是分类问题上，不同标签的样本比例很可能是不均衡的。因此，如果直接使用算法训练进行分类，训练效果可能会很差。

二、如何解决数据不平衡问题
解决实际应用中数据不平衡问题可以从三个方面入手，分别是对数据进行处理、选择合适的评估方法和使用合适的算法。

· 数据处理
1）过采样：

    主动获取更多的比例少的样本数据。由于样本比例不均衡，在条件允许的情况下可以尝试获取占比少的类型的样本数据。(PS：这就是为什么我几乎没有遇到过数据不平衡的问题。每次测试使用的数据集都尽可能的完美均衡) 也可以通过使用重复、自举或合成少数类过采样等方法（SMOTE）来生成新的稀有样品。

    直接简单复制重复的话，如果特征少，会导致过拟合的问题。经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本 (数据增强)。

2）欠采样：

    数据量足够时，可以通过保留比例小的样本数据和减少比例大的样本数据来平衡数据集。缺点是会丢失多数类中的一些重要信息。

3）改变权重：

    对不同样本数量的类别赋予不同的权重（通常会设置为与样本量成反比）

4）使用K-fold交叉验证

    值得注意的是，使用过采样方法来解决不平衡问题时应适当地应用交叉验证。这是因为过采样会观察到罕见的样本，并根据分布函数应用自举生成新的随机数据，如果在过采样之后应用交叉验证，那么我们所做的就是将我们的模型过拟合于一个特定的人工引导结果。这就是为什么在过度采样数据之前应该始终进行交叉验证，就像实现特征选择一样。只有重复采样数据可以将随机性引入到数据集中，以确保不会出现过拟合问题。

    K-fold交叉验证就是把原始数据随机分成K个部分，在这K个部分中选择一个作为测试数据，剩余的K-1个作为训练数据。交叉验证的过程实际上是将实验重复做K次，每次实验都从K个部分中选择一个不同的部分作为测试数据，剩余的数据作为训练数据进行实验，最后把得到的K个实验结果平均。

    此外，还应注意训练集和测试集的样本的概率分布问题。若实际数据不平衡，将采样平衡后的数据集作为训练集训练后，模型应用在测试集上效果仍会不好。因此，实际应用中尽可能保持训练和测试的样本的概率分布是一致的。

· 选择合适的评价指标
1）谨慎选择AUC作为评价指标：对于数据极端不平衡时，可以观察观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少量样本的效果其实并不理想的情况。

2）不要只看Accuracy：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，在实际生产中更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看。

· 选择合适的算法
1）选择对数据倾斜相对不敏感的算法。如树模型等。

2）集成学习。即多模型Bagging。首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，再加权组成新的分类器，如加法模型、Adaboost、随机森林等。

3）转化成异常检测或者一分类问题。(具体内容后续有时间再跟进学习)

------------------------------------------------------------------------------------------------------------------------------------------------

2、激活函数部分【有哪些激活函数，属性，优缺点，适用场景】
为什么使用激活函数？
a、如果不用激活函数，那么无论神经网络层数多深，最终都相当于一个线性函数
b、引入激活函数可以增强模型的表征能力，提高模型的非线性能力。
c、性质：非线性、可微性、单调性保证损失函数是凸函数
c、sigmoid
特点：
它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
缺点：
sigmoid函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它固有的一些 缺点。
缺点1：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。首先来看Sigmoid函数的导数，如下图所示：
如果我们初始化神经网络的权值为 [ 0 , 1 ] [0,1][0,1] 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。
详细数学分析见文章：http://neuralnetworksanddeeplearning.com/chap5.html 中文译文：深度神经网络为何很难训练
缺点2：Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如x &gt; 0 ,   f = w T x + b x&gt;0, \ f= w^Tx+bx>0, f=w 
T
 x+b,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
缺点3：其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。

应用场景：二分类模型的输出，门控循环单元

tanch
它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。
应用场景：隐藏层：例如隐藏状态和记忆状态

relu
ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
1） 解决了gradient vanishing问题 (在正区间)
2）计算速度非常快，只需要判断输入是否大于0
3）收敛速度远快于sigmoid和tanh

ReLU也有几个需要特别注意的问题：
1）ReLU的输出不是zero-centered
2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
应用场景：隐藏层激活函数

leakey relu
人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为α x \alpha xαx而非0，通常α = 0.01 \alpha=0.01α=0.01。另外一种直观的想法是基于参数的方法，即P a r a m e t r i c R e L U : f ( x ) = max ⁡ ( α x , x ) Parametric ReLU:f(x) = \max(\alpha x, x)ParametricReLU:f(x)=max(αx,x)，其中α \alphaα
可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU

elu
ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：

不会有Dead ReLU问题
输出的均值接近0，zero-centered
1不会有Dead ReLU问题
2输出的均值接近0，zero-centered
它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。

尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试
---------------------------------------------------------------------------------------------------------------------------------------------------------
损失函数
损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。
0-1损失是指预测值和目标值不相等为1， 否则为0:

[公式]

特点：

(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2)感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 [公式] 时认为相等，

[公式]
绝对值损失函数是计算预测值与目标值的差的绝对值：

[公式]
log对数损失函数的标准形式如下：

[公式]

特点：

(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。

(2)健壮性不强，相比于hinge loss对噪声更敏感。

(3)逻辑回归的损失函数就是log对数损失函数。

Hinge损失函数标准形式如下：

[公式]

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 [公式] 。SVM就是使用这个损失函数。

(2)一般的 [公式] 是预测值，在-1到1之间， [公式] 是目标值(-1或1)。其含义是， [公式] 的值在-1和+1之间就可以了，并不鼓励 [公式] ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。

(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。

指数损失函数的标准形式如下：

[公式]

特点：

(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。

感知损失函数的标准形式如下：

[公式]

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。

交叉熵损失函数的标准形式如下:

[公式]

注意公式中 [公式] 表示样本， [公式] 表示实际的标签， [公式] 表示预测的输出， [公式] 表示样本总数量。

特点：

(1)本质上也是一种对数似然函数，可用于二分类和多分类任务中。

二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

[公式]

多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

[公式]

(2)当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

最后奉献上交叉熵损失函数的实现代码：cross_entropy.



这里需要更正一点，对数损失函数和交叉熵损失函数应该是等价的！！！（此处感谢 
@Areshyy
 的指正，下面说明也是由他提供）

下面来具体说明：


相关高频问题：

1.交叉熵函数与最大似然函数的联系和区别？

区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说最小化交叉熵函数的本质就是对数似然函数的最大化。

怎么推导的呢？我们具体来看一下。

设一个随机变量 [公式] 满足伯努利分布，

[公式]

则 [公式] 的概率密度函数为：

[公式]

因为我们只有一组采样数据 [公式] ，我们可以统计得到 [公式] 和 [公式] 的值，但是 [公式] 的概率是未知的，接下来我们就用极大似然估计的方法来估计这个 [公式] 值。

对于采样数据 [公式] ，其对数似然函数为:

[公式]

可以看到上式和交叉熵函数的形式几乎相同，极大似然估计就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到交叉熵函数（或交叉熵损失函数）：

[公式]

这个式子揭示了交叉熵函数与极大似然估计的联系，最小化交叉熵函数的本质就是对数似然函数的最大化。

现在我们可以用求导得到极大值点的方法来求其极大似然估计，首先将对数似然函数对 [公式] 进行求导，并令导数为0，得到

[公式]

消去分母，得：

[公式]

所以:

[公式]

这就是伯努利分布下最大似然估计求出的概率 [公式] 。

2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？

其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。

对于均方误差损失函数，常常定义为：

[公式]

其中 [公式] 是我们期望的输出， [公式] 为神经元的实际输出（ [公式] ）。在训练神经网络的时候我们使用梯度下降的方法来更新 [公式] 和 [公式] ，因此需要计算代价函数对 [公式] 和 [公式] 的导数：

[公式]

然后更新参数 [公式] 和 [公式] ：

[公式]

因为sigmoid的性质，导致 [公式] 在 [公式] 取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 [公式] 很小，导致参数 [公式] 和 [公式] 更新非常慢。


那么为什么交叉熵损失函数就会比较好了呢？同样的对于交叉熵损失函数，计算一下参数更新的梯度公式就会发现原因。交叉熵损失函数一般定义为：

[公式]

其中 [公式] 是我们期望的输出， [公式] 为神经元的实际输出（ [公式] ）。同样可以看看它的导数：

[公式]

另外，

[公式]

所以有：

[公式]

[公式]

所以参数更新公式为：

[公式]

可以看到参数更新公式中没有 [公式] 这一项，权重的更新受 [公式] 影响，受到误差的影响，所以当误差大的时候，权重更新快；当误差小的时候，权重更新慢。这是一个很好的性质。

所以当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数。


