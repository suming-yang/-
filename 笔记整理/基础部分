1、如何处理不平衡数据?
通常，实际数据和标准数据集在数据集的属性和数据量方面差异很大(标准数据集不需要调整)。对于实际的数据集，可能会出现数据不平衡的情况，即类之间的数据不平衡。我们现在可以考虑以下技术：

a、选择正确的度量来评估模型：对于不平衡的数据集，使用准确性来评估是一项非常危险的工作。应选择精度、召回、F1 分数、AUC等合适的评价量。
b、重新采样训练数据集：除了使用不同的评估标准，人们还可以使用一些技术来获得不同的数据集。从一个不平衡的数据集中创建一个平衡的数据集有两种方法，即欠采样和过采样，具体技术包括重复、
   bootstrapping 或 hits(综合少数过采样技术)等方法。
c、许多不同模型的集成：通过创建更多的数据来概括模型在实践中并不总是可行的。例如，你有两个层，一个拥有 1000 个数据的罕见类，一个包含 10,000 个数据样本的大型类。因此，我们可以考虑一个 10 个模型的训练解决方案，而不是试图从一个罕见的类中找到 9000 个数据样本来进行模型训练。每个模型由 1000 个稀有类和 1000 个大规模类训练而成。然后使用集成技术获得优秀结果。
d、重新设计模型 — 损失函数：使用惩罚技术对代价函数中的多数类进行严厉惩罚，帮助模型本身更好地学习稀有类的数据。这使得损失函数的值在类中更全面。

一、什么是数据不平衡问题
数据不平衡也可称作数据倾斜。在实际应用中，数据集的样本特别是分类问题上，不同标签的样本比例很可能是不均衡的。因此，如果直接使用算法训练进行分类，训练效果可能会很差。

二、如何解决数据不平衡问题
解决实际应用中数据不平衡问题可以从三个方面入手，分别是对数据进行处理、选择合适的评估方法和使用合适的算法。

· 数据处理
1）过采样：

    主动获取更多的比例少的样本数据。由于样本比例不均衡，在条件允许的情况下可以尝试获取占比少的类型的样本数据。(PS：这就是为什么我几乎没有遇到过数据不平衡的问题。每次测试使用的数据集都尽可能的完美均衡) 也可以通过使用重复、自举或合成少数类过采样等方法（SMOTE）来生成新的稀有样品。

    直接简单复制重复的话，如果特征少，会导致过拟合的问题。经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本 (数据增强)。

2）欠采样：

    数据量足够时，可以通过保留比例小的样本数据和减少比例大的样本数据来平衡数据集。缺点是会丢失多数类中的一些重要信息。

3）改变权重：

    对不同样本数量的类别赋予不同的权重（通常会设置为与样本量成反比）

4）使用K-fold交叉验证

    值得注意的是，使用过采样方法来解决不平衡问题时应适当地应用交叉验证。这是因为过采样会观察到罕见的样本，并根据分布函数应用自举生成新的随机数据，如果在过采样之后应用交叉验证，那么我们所做的就是将我们的模型过拟合于一个特定的人工引导结果。这就是为什么在过度采样数据之前应该始终进行交叉验证，就像实现特征选择一样。只有重复采样数据可以将随机性引入到数据集中，以确保不会出现过拟合问题。

    K-fold交叉验证就是把原始数据随机分成K个部分，在这K个部分中选择一个作为测试数据，剩余的K-1个作为训练数据。交叉验证的过程实际上是将实验重复做K次，每次实验都从K个部分中选择一个不同的部分作为测试数据，剩余的数据作为训练数据进行实验，最后把得到的K个实验结果平均。

    此外，还应注意训练集和测试集的样本的概率分布问题。若实际数据不平衡，将采样平衡后的数据集作为训练集训练后，模型应用在测试集上效果仍会不好。因此，实际应用中尽可能保持训练和测试的样本的概率分布是一致的。

· 选择合适的评价指标
1）谨慎选择AUC作为评价指标：对于数据极端不平衡时，可以观察观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少量样本的效果其实并不理想的情况。

2）不要只看Accuracy：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，在实际生产中更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看。

· 选择合适的算法
1）选择对数据倾斜相对不敏感的算法。如树模型等。

2）集成学习。即多模型Bagging。首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，再加权组成新的分类器，如加法模型、Adaboost、随机森林等。

3）转化成异常检测或者一分类问题。(具体内容后续有时间再跟进学习)

------------------------------------------------------------------------------------------------------------------------------------------------

2、激活函数部分【有哪些激活函数，属性，优缺点，适用场景】
为什么使用激活函数？
a、如果不用激活函数，那么无论神经网络层数多深，最终都相当于一个线性函数
b、引入激活函数可以增强模型的表征能力，提高模型的非线性能力。
c、性质：非线性、可微性、单调性保证损失函数是凸函数
c、sigmoid
特点：
它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
缺点：
sigmoid函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它固有的一些 缺点。
缺点1：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。首先来看Sigmoid函数的导数，如下图所示：
如果我们初始化神经网络的权值为 [ 0 , 1 ] [0,1][0,1] 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 ( 1 , + ∞ ) (1,+∞)(1,+∞) 区间内的值，则会出现梯度爆炸情况。
详细数学分析见文章：http://neuralnetworksanddeeplearning.com/chap5.html 中文译文：深度神经网络为何很难训练
缺点2：Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如x &gt; 0 ,   f = w T x + b x&gt;0, \ f= w^Tx+bx>0, f=w 
T
 x+b,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
缺点3：其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。

应用场景：二分类模型的输出，门控循环单元

tanch
它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。
应用场景：隐藏层：例如隐藏状态和记忆状态

relu
ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
1） 解决了gradient vanishing问题 (在正区间)
2）计算速度非常快，只需要判断输入是否大于0
3）收敛速度远快于sigmoid和tanh

ReLU也有几个需要特别注意的问题：
1）ReLU的输出不是zero-centered
2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
应用场景：隐藏层激活函数

leakey relu
人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为α x \alpha xαx而非0，通常α = 0.01 \alpha=0.01α=0.01。另外一种直观的想法是基于参数的方法，即P a r a m e t r i c R e L U : f ( x ) = max ⁡ ( α x , x ) Parametric ReLU:f(x) = \max(\alpha x, x)ParametricReLU:f(x)=max(αx,x)，其中α \alphaα
可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU

elu
ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：

不会有Dead ReLU问题
输出的均值接近0，zero-centered
1不会有Dead ReLU问题
2输出的均值接近0，zero-centered
它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。

尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试


